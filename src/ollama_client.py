"""
Client for interacting with the Ollama API.
"""

import json
import logging
from typing import Dict, Any, Optional, List

import httpx

from src.config import OllamaConfig

logger = logging.getLogger("ollama-ghidra-bridge.ollama")

class OllamaClient:
    """Client for interacting with Ollama API."""
    
    def __init__(self, config: OllamaConfig):
        """
        Initialize the Ollama client.
        
        Args:
            config: OllamaConfig object with connection details
        """
        self.config = config
        self.generate_url = f"{config.base_url}/api/generate"
        self.chat_url = f"{config.base_url}/api/chat"
        self.client = httpx.Client(timeout=config.timeout)
        logger.info(f"Initialized Ollama client with model: {config.model}")
        
        # Log any phase-specific models that are configured
        for phase, model in self.config.model_map.items():
            if model:
                logger.info(f"Using specialized model for {phase} phase: {model}")
    
    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        Send a prompt to the Ollama model and get a response.
        
        Args:
            prompt: The user prompt to send to the model
            system_prompt: Optional system prompt to guide the model
            
        Returns:
            The model's response as a string
            
        Raises:
            Exception: If the request fails
        """
        # Call the chat API with tool support for models that support it
        try:
            return self._chat_with_tools(self.config.model, prompt, system_prompt)
        except Exception as e:
            logger.warning(f"Tool calling failed, falling back to generate API: {str(e)}")
            return self._generate_with_model(self.config.model, prompt, system_prompt)
    
    def _chat_with_tools(self, model: str, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        Send a prompt to the Ollama chat API with tool support.
        
        Args:
            model: The model to use
            prompt: The user prompt to send to the model
            system_prompt: Optional system prompt to guide the model
            
        Returns:
            The model's response as a string
            
        Raises:
            Exception: If the request fails
        """
        messages = [{"role": "user", "content": prompt}]
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "tools": self.config.tools
        }
        
        if system_prompt:
            payload["system"] = system_prompt
        
        try:
            logger.debug(f"Sending chat request to Ollama model '{model}' with tools: {prompt[:100]}...")
            response = self.client.post(self.chat_url, json=payload)
            response.raise_for_status()
            
            result = response.json()
            
            # Handle tool calls if present
            if "message" in result and "tool_calls" in result["message"]:
                tool_calls = result["message"]["tool_calls"]
                logger.info(f"Received {len(tool_calls)} tool calls from model")
                
                # Extract tool calls in a format compatible with our command parser
                commands = []
                for tool_call in tool_calls:
                    if tool_call["type"] == "function":
                        function = tool_call["function"]
                        name = function["name"]
                        params = json.loads(function["arguments"])
                        commands.append((name, params))
                
                # Format the commands as executable commands (for backward compatibility)
                formatted_commands = []
                for name, params in commands:
                    params_str = ", ".join([f'{k}="{v}"' for k, v in params.items()])
                    formatted_commands.append(f"EXECUTE: {name}({params_str})")
                
                # Add any explanation text from the model
                text_response = result["message"].get("content", "")
                if text_response:
                    return text_response + "\n\n" + "\n".join(formatted_commands)
                else:
                    return "\n".join(formatted_commands)
            else:
                # Regular text response
                if "message" in result and "content" in result["message"]:
                    return result["message"]["content"]
                else:
                    return "No response generated by the model."
        except Exception as e:
            logger.error(f"Error with chat API: {str(e)}")
            raise
    
    def generate_with_phase(self, prompt: str, phase: str = None, system_prompt: Optional[str] = None) -> str:
        """
        Send a prompt to the Ollama API with a specific phase.
        Uses the appropriate model and system prompt for the given phase.
        
        Args:
            prompt: The user prompt to send to the model
            phase: The phase of the agent process (planning, execution, analysis)
            system_prompt: Optional system prompt to override the default
            
        Returns:
            The model's response as a string
            
        Raises:
            Exception: If the request fails
        """
        # Get phase-appropriate model if specified in model_map
        model = self.config.model
        if phase and phase in self.config.model_map and self.config.model_map[phase]:
            model = self.config.model_map[phase]
            logger.info(f"Using specialized model for {phase} phase: {model}")
        
        # Get phase-appropriate system prompt
        phase_system_prompt = None
        
        # First check if there's a specific override in phase_system_prompts
        if system_prompt is None and phase in self.config.phase_system_prompts and self.config.phase_system_prompts[phase]:
            phase_system_prompt = self.config.phase_system_prompts[phase]
            logger.info(f"Using custom override system prompt for {phase} phase")
        # Then check for the dedicated phase-specific prompt attribute
        elif system_prompt is None:
            if phase == "planning" and hasattr(self.config, "planning_system_prompt"):
                phase_system_prompt = self.config.planning_system_prompt
                logger.info(f"Using planning system prompt")
            elif phase == "execution" and hasattr(self.config, "execution_system_prompt"):
                phase_system_prompt = self.config.execution_system_prompt
                logger.info(f"Using execution system prompt")
            elif phase == "analysis" and hasattr(self.config, "analysis_system_prompt"):
                phase_system_prompt = self.config.analysis_system_prompt
                logger.info(f"Using analysis system prompt")
        
        # Use provided system_prompt, or phase-specific one, or default
        final_system_prompt = system_prompt or phase_system_prompt or self.config.default_system_prompt
        
        # Try with chat API first, fall back to generate API
        try:
            return self._chat_with_tools(model, prompt, final_system_prompt)
        except Exception as e:
            logger.warning(f"Tool calling failed for phase {phase}, falling back to generate API: {str(e)}")
            return self._generate_with_model(model, prompt, final_system_prompt)
    
    def _generate_with_model(self, model: str, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        Internal method to send a prompt to a specific Ollama model using the generate API.
        
        Args:
            model: The specific model to use
            prompt: The user prompt to send to the model
            system_prompt: Optional system prompt to guide the model
            
        Returns:
            The model's response as a string
            
        Raises:
            Exception: If the request fails
        """
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False  # Explicitly disable streaming to get a single JSON response
        }
        
        if system_prompt:
            payload["system"] = system_prompt
        
        try:
            logger.debug(f"Sending prompt to Ollama model '{model}' using generate API: {prompt[:100]}...")
            response = self.client.post(self.generate_url, json=payload)
            response.raise_for_status()
            
            # Handle the response based on content
            content_type = response.headers.get('content-type', '')
            if 'application/json' in content_type:
                try:
                    # Try to parse as a single JSON object
                    result = response.json()
                    logger.debug(f"Received response from Ollama: {result.get('response', '')[:100]}...")
                    return result.get("response", "")
                except json.JSONDecodeError:
                    # If it fails, try to parse as multiple JSON objects (stream)
                    response_text = response.text
                    # Handle case where we have multiple JSON objects (one per line)
                    lines = response_text.strip().split('\n')
                    if len(lines) > 1:
                        # Combine all responses in the stream
                        combined_response = ""
                        for line in lines:
                            try:
                                line_json = json.loads(line)
                                combined_response += line_json.get("response", "")
                            except json.JSONDecodeError as e:
                                logger.warning(f"Could not parse JSON line: {line}, error: {str(e)}")
                        return combined_response
                    else:
                        # If we can't handle it, just return the raw text
                        logger.warning("Could not parse JSON response, returning raw text")
                        return response_text
            else:
                # If not JSON, just return the text
                logger.warning(f"Received non-JSON response with content-type: {content_type}")
                return response.text
        except Exception as e:
            logger.error(f"Error communicating with Ollama: {str(e)}")
            raise
    
    def list_models(self) -> List[str]:
        """
        List available models on the Ollama server.
        
        Returns:
            List of model names
            
        Raises:
            Exception: If the request fails
        """
        try:
            response = self.client.get(f"{self.config.base_url}/api/tags")
            response.raise_for_status()
            result = response.json()
            models = [model.get("name") for model in result.get("models", [])]
            logger.info(f"Retrieved {len(models)} available models from Ollama")
            return models
        except Exception as e:
            logger.error(f"Error listing Ollama models: {str(e)}")
            raise

    def health_check(self) -> bool:
        """
        Check if the Ollama server is available.
        
        Returns:
            True if the server is available, False otherwise
        """
        try:
            response = self.client.get(f"{self.config.base_url}")
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Ollama server health check failed: {str(e)}")
            return False 